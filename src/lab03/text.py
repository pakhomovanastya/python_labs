import re
def normalize(text: str, *, casefold: bool = True, yo2e: bool = True) -> str:
    if casefold:
        text = text.casefold() 
        '''casefold() –ª—É—á—à–µ —á–µ–º lower(), –ø–æ—Ç–æ–º—É —á—Ç–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —è–∑—ã–∫–∞–º–∏'''
    if yo2e:
        text = text.replace('–Å', '–ï')
        text = text.replace('—ë', '–µ')
    text = text.strip()
    text = re.sub(r'\\[trnwfdv]', ' ', text)
    '''–ù–∞—Ö–æ–¥–∏—Ç —Å–∏–º–≤–æ–ª—ã —Ç–∏–ø–∞ \t, \n, \r –∏ –∑–∞–º–µ–Ω—è–µ—Ç –∏—Ö –Ω–∞ –æ–±—ã—á–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã.
    –≠—Ç–∏ —Å–∏–º–≤–æ–ª—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–æ–≤ —Å—Ç—Ä–æ–∫, —Ç–∞–±—É–ª—è—Ü–∏–∏ –∏ —Ç.–¥.'''
    text = ' '.join(text.split())
    return text

def tokenize(text: str) -> list[str]:
    text_1 = text.replace(',', ' ').replace('.', ' ').replace(':', ' ').replace(';', ' ').replace('!', ' ').replace('?', ' ')
    text_1 = text_1.split()
    b = []
    for i in text_1:
        if i[0].isdigit() == 0 and i[0].isalpha() == 0: 
            pass ## –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π —Å–∏–º–≤–æ–ª "–ø–ª–æ—Ö–æ–π" (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–º–∞–π–ª–∏–∫), —Å–ª–æ–≤–æ –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è
        # i[0]== 0 —Ç–∫ –º—ã –ø—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–ª—è–µ—Ç—å—Å—è –ª–∏ i —á–∏—Å–ª–æ–º\–±—É–∫–≤–æ–π,
        # 0 –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–∞–≤–¥–∞ —ç—Ç–æ –∏–ª–∏ –Ω–µ—Ç. –ï—Å–ª–∏ fals, —Ç–æ –Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º 
        # –≤ —Å–ø–∏—Å–æ–∫. pass –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç i –∏ –Ω–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç if
        else:
            b.append(i)
    return b

def count_freq(tokens: list[str]) -> dict[str, int]:
    unic_tokens = list(set(tokens))
    '''–ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–æ –º–Ω–æ–∂–µ—Å—Ç–≤–æ –æ–±—Ä–∞—Ç–Ω–æ –≤ —Å–ø–∏—Å–æ–∫,
    –¥–∞–ª–µ–µ –ø–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤'''
    count_tokens = []
    for i in unic_tokens:
        count_tokens.append(tokens.count(i))
    dict_tokens = dict(zip(unic_tokens,count_tokens))
    '''"—Å—à–∏–≤–∞–µ—Ç" –¥–≤–∞ —Å–ø–∏—Å–∫–∞ –≤–º–µ—Å—Ç–µ [("a", 3), ("b", 2), ("c", 1)]
    dict(...) - –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–∏ –ø–∞—Ä—ã –≤ —Å–ª–æ–≤–∞—Ä—å {"a": 3, "b": 2, "c": 1}'''
    return dict_tokens

# def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
#     freq = count_freq(freq)
#     new_dict = freq.items()
#     top = sorted(new_dict, key=lambda x: x[0])
#     '''–º—ã —Å–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ new_dict –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—É (x[0]) —Å –ø–æ–º–æ—â—å—é key
#     –≤ key –º—ã –ø–µ—Ä–µ–¥–∞—ë–º –ø–∞—Ä–º–∞–µ—Ç—Ä —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞'''
#     ans = sorted(top, key=lambda x: -x[1])[:n]
#     return ans

def top_n(freq: dict[str, int], n: int = 5) -> list[tuple[str, int]]:
    freq = count_freq(freq)
    top = sorted(list(freq.items()), key=lambda x: (-x[1], x[0])) [:n] # items –≤—ã–≤–æ–¥–∏—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π –≤ —Ñ–æ—Ä–º–∞—Ç–µ (–∫–ª—é—á, –∑–Ω–∞—á–µ–Ω–∏–µ) 
    '''key=lambda x: x[0] –ø—Ä–∞–≤–∏–ª–æ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –∏ [:n] —Å—Ä–µ–∑, –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ n —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    lambda x - —Å–æ–∑–¥–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é
    (-x[1], x[0]) - –∫–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤:
    -x[1] - –º–∏–Ω—É—Å –ø–µ—Ä–µ–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —á–∞—Å—Ç–æ—Ç—ã)
    x[0] - —Å–ª–æ–≤–æ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç–∞)
    –¥–∞–ª–µ–µ —Å–æ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é'''
    return top

test_case_normalize = ['–ü—Ä–ò–≤–ï—Ç\n–ú–ò—Ä\t', '—ë–∂–∏–∫, –Å–ª–∫–∞', 'Hello\r\nWorld', '  –¥–≤–æ–π–Ω—ã–µ   –ø—Ä–æ–±–µ–ª—ã  ']
test_case_tokenize = ['–ø—Ä–∏–≤–µ—Ç –º–∏—Ä', 'hello,world!!!', '–ø–æ-–Ω–∞—Å—Ç–æ—è—â–µ–º—É –∫—Ä—É—Ç–æ', '2025 –≥–æ–¥', 'emoji üòÄ –Ω–µ —Å–ª–æ–≤–æ']
test_case_count_freq =[["a","b","a","c","b","a"]]


for i in test_case_normalize:
    print(normalize(i))

for n in test_case_tokenize:
    print(tokenize(n))

for n in test_case_count_freq:
    print(top_n(n))